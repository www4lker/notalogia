---
{"dg-publish":true,"permalink":"/0-vault/1-notas-literais/gerais-interesses/como-o-computador-aprende/","tags":["interessesgerais","interessesdepesquisa","meta","promptgpt3"],"dgHomeLink":true,"dgShowLocalGraph":true,"dgShowFileTree":true,"dgEnableSearch":true,"noteIcon":""}
---

# 131220222146
## criado em: 21:46 2022-12-13

### Relacionado
- palavras-chave: #interessesgerais  #interessesdepesquisa #meta #promptgpt3  
- notas: 
- [[0 - VAULT/1 NOTAS LITERAIS/Interesses de Pesquisa/wired with a coder\|wired with a coder]]
- [[0 - VAULT/1 NOTAS LITERAIS/Gerais, Interesses/peso neural\|peso neural]]
- [[0 - VAULT/2 NOTAS PERMANENTES/gpt3\|gpt3]]
- [[0 - VAULT/2 NOTAS PERMANENTES/gpt3 por ele mesmo\|gpt3 por ele mesmo]]
---
# como o computador “aprende”?

>Backpropagation é um método usado para treinar redes neurais artificiais. É uma técnica para atualizar os pesos da rede para reduzir o erro entre a saída prevista e a saída real. *Os pesos* são atualizados com base no gradiente de erro, que é calculado usando as derivadas parciais da função de erro em relação a cada *peso*. Este processo é chamado de backpropagation porque o gradiente de erro é "propagado" para trás através da rede, começando pela camada de saída e trabalhando para trás através das camadas ocultas até chegar à camada de entrada. Os pesos são então atualizados para reduzir o erro na saída prevista. Desta forma, o backpropagation permite que a rede aprenda com os dados de treinamento e melhore seu desempenho em entradas futuras.